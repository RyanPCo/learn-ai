{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "730bcdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "USE_GPU = True\n",
    "dtype = torch.float32\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print_every = 100\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5431c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x, context=None, mask=None):\n",
    "        if context is None:\n",
    "            context = x\n",
    "        out, _ = self.attn(x, context, context, attn_mask=mask)\n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        out = self.fc2(x)\n",
    "        return out\n",
    "    \n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = x + identity\n",
    "\n",
    "        identity = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        out = x + identity\n",
    "        return out\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=64):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros((1, max_len, d_model))\n",
    "        i = torch.arange(max_len)[:, None]\n",
    "        pows = torch.pow(10000, -torch.arange(0, d_model, 2) / d_model)\n",
    "\n",
    "        pe[0, :, 0::2] = torch.sin(i * pows)\n",
    "        pe[0, :, 1::2] = torch.cos(i * pows)\n",
    "\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N = x.size(1)\n",
    "        pe = self.pe[:, :N, :]\n",
    "        return x + pe\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.pe = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderBlock(d_model, num_heads, d_ff)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pe(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        out = self.norm(x)\n",
    "        return out\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.attn2 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, enc_out, mask):\n",
    "        identity = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn1(x, mask=mask)\n",
    "        x = x + identity\n",
    "\n",
    "        identity = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.attn2(x, context=enc_out)\n",
    "        x = x + identity\n",
    "\n",
    "        identity = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.ff(x)\n",
    "        out = x + identity\n",
    "        return out\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.pe = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(d_model, num_heads, d_ff)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, enc_out):\n",
    "        seq_len = x.size(1)\n",
    "        x = self.pe(x)\n",
    "\n",
    "        mask = self.casual_mask(seq_len, x.device)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, mask)\n",
    "\n",
    "        out = self.norm(x)\n",
    "        return out\n",
    "    \n",
    "    def casual_mask(self, size, device):\n",
    "        mask = torch.triu(torch.ones(size, size, device=device), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, d_out):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, d_ff)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, d_ff)\n",
    "        self.fc = nn.Linear(d_model, d_out)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        enc_out = self.encoder(src)\n",
    "        dec_out = self.decoder(tgt, enc_out)\n",
    "        out = self.fc(dec_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9853ef2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 10])\n"
     ]
    }
   ],
   "source": [
    "B = 2\n",
    "src_len = 5\n",
    "tgt_len = 4\n",
    "d_model = 16\n",
    "d_out = 10\n",
    "\n",
    "src = torch.randn(B, src_len, d_model)\n",
    "tgt = torch.randn(B, tgt_len, d_model)\n",
    "\n",
    "model = Transformer(num_layers=2, d_model=d_model, num_heads=4, d_ff=64, d_out=d_out)\n",
    "\n",
    "out = model(src, tgt)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d5f150c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, num_samples=1000, seq_len=8, d_model=16):\n",
    "        self.data = torch.randn(num_samples, seq_len, d_model)\n",
    "        self.targets = torch.flip(self.data, dims=[1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "    \n",
    "dataset = ReverseDataset(num_samples=2000, seq_len=8, d_model=d_model)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "149feac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, epochs):\n",
    "    model = model.to(device=device)\n",
    "    for e in range(epochs):\n",
    "        total_loss = 0\n",
    "        for (src, tgt_out) in loader:\n",
    "            bos = torch.zeros(src.size(0), 1, d_model)\n",
    "            tgt_in = torch.cat([bos, tgt_out[:, :-1, :]], dim=1)\n",
    "\n",
    "            pred = model(src, tgt_in)\n",
    "            loss = criterion(pred, tgt_out)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {e+1}, loss={total_loss/len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ecd39d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss=1.2791\n",
      "Epoch 2, loss=1.1713\n",
      "Epoch 3, loss=1.0997\n",
      "Epoch 4, loss=1.0588\n",
      "Epoch 5, loss=1.0381\n",
      "Epoch 6, loss=1.0282\n",
      "Epoch 7, loss=1.0205\n",
      "Epoch 8, loss=1.0159\n",
      "Epoch 9, loss=1.0124\n",
      "Epoch 10, loss=1.0096\n"
     ]
    }
   ],
   "source": [
    "B = 2\n",
    "src_len = 5\n",
    "tgt_len = 4\n",
    "d_model = 16\n",
    "d_out = 16\n",
    "\n",
    "src = torch.randn(B, src_len, d_model)\n",
    "tgt = torch.randn(B, tgt_len, d_model)\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model = Transformer(num_layers=2, d_model=d_model, num_heads=4, d_ff=64, d_out=d_out)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train(model, optimizer, criterion, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
